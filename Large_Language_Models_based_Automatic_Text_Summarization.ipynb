{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1WRedBvPyhrb0T3PP3Rn8XCH8j1s5FA4p",
      "authorship_tag": "ABX9TyMQmBL5LLfaLPGSaga2jDYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamaskecskemeti/nlp_thesis/blob/main/Large_Language_Models_based_Automatic_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5ODgb8jYkTY"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install bert_score\n",
        "!pip install meteor_score\n",
        "!pip install gradio\n",
        "!pip install bitsandbytes\n",
        "!pip install --upgrade transformers accelerate bitsandbytes\n",
        "!pip3 install --upgrade trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import itertools\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Gemma3ForCausalLM, BitsAndBytesConfig # AutoModelForSeq2SeqLM\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import gradio as gr\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import psutil\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ZywxCYQHYnZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "5k6KIyJU_UQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "0xVDIPzEYosR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
        "compute_dtype"
      ],
      "metadata": {
        "id": "1rWCAD9gwENU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login and mount\n",
        "hf_token = \"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\"\n",
        "login(token=hf_token,add_to_git_credential=True)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "GT1wqTXpYpA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)"
      ],
      "metadata": {
        "id": "8FrlL17_YtbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"news_and_summaries.csv\", sep=',')\n",
        "dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "DbRnfKNcYuwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the 80-20 train-holdout split\n",
        "train_holdout_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_holdout_split['train']\n",
        "holdout_dataset = train_holdout_split['test']"
      ],
      "metadata": {
        "id": "HgPPpx4adfoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpu_memory():\n",
        "    allocated = torch.cuda.memory_allocated() / 1e6\n",
        "    reserved = torch.cuda.memory_reserved() / 1e6\n",
        "    return round(allocated, 1), round(reserved, 1)\n",
        "\n",
        "def get_ram_usage():\n",
        "    vm = psutil.virtual_memory()\n",
        "    return round(vm.used / 1e9, 2), round(vm.percent, 1)\n",
        "\n",
        "def log_resources(label=\"\"):\n",
        "    gpu_alloc, gpu_reserved = get_gpu_memory()\n",
        "    ram_used, ram_pct = get_ram_usage()\n",
        "    print(f\"\\n{label} Resources:\")\n",
        "    print(f\"   GPU Allocated: {gpu_alloc} MB\")\n",
        "    print(f\"   GPU Reserved:  {gpu_reserved} MB\")\n",
        "    print(f\"   RAM Used:      {ram_used} GB ({ram_pct}%)\\n\")"
      ],
      "metadata": {
        "id": "QT6E54AzAcjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(examples):\n",
        "    inputs = [\"You are a helpful assistant.\\nBelow is a political text. Summarize it in a few concise sentences and return only the summary.\\n\\nText:\\n\\n{text}\\n\\nSummary:\" + doc for doc in examples['text']]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Tokenize summaries\n",
        "    labels = tokenizer(examples['summary'], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # Ensure padding tokens are ignored in the loss calculation\n",
        "    model_inputs[\"labels\"] = [\n",
        "      [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
        "      for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "YWOWBK33Txd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OLD Function to generate summary\n",
        "## def generate_summary(text):\n",
        "##     inputs = tokenizer(f\"\\nBelow is a long political text. Summarize it in a few sentences and return only the summary. I repeat, only the summary!\\n\\nText:\\n\\n{text}\\n\\n. The summary of this text, without the input text:\", return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "##     summary_ids = model.generate(inputs['input_ids'],\n",
        "##                                  max_length=256,\n",
        "##                                  min_length=64,\n",
        "##                                  num_beams=4,\n",
        "##                                  temperature=0.8,\n",
        "##                                  top_k=50,\n",
        "##                                  top_p=0.9,\n",
        "##                                  repetition_penalty=1.5,\n",
        "##                                  no_repeat_ngram_size=4,\n",
        "##                                  early_stopping=True)\n",
        "##     decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "##\n",
        "##     # summary = remove_copied_sections(decoded, text)\n",
        "##     # Strip the summary so only the part after the \"Text:\" is kept\n",
        "##     if \"Text:\" in decoded:\n",
        "##         summary = decoded.split(\"The summary of this text:\")[-1].strip()\n",
        "##     else:\n",
        "##         summary = decoded.strip()\n",
        "##     return summary"
      ],
      "metadata": {
        "id": "91EWE8u9TfYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk text to paragraphs and make pairs out of them\n",
        "def chunk_by_paragraphs(text, min_len=50):\n",
        "    # Split by paragraph breaks\n",
        "    paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 0]\n",
        "\n",
        "    # Filter short paragraphs\n",
        "    paragraphs = [p for p in paragraphs if len(p) > min_len]\n",
        "\n",
        "    # Group paragraphs in pairs\n",
        "    paired_chunks = []\n",
        "    for i in range(0, len(paragraphs), 2):\n",
        "        pair = paragraphs[i]\n",
        "        if i + 1 < len(paragraphs):\n",
        "            pair += \"\\n\\n\" + paragraphs[i + 1]\n",
        "        paired_chunks.append(pair)\n",
        "\n",
        "    return paired_chunks\n",
        "\n",
        "# Summarize paragraphs pairs\n",
        "def summarize_paragraphs(model, tokenizer, text, device='cuda'):\n",
        "    chunks = chunk_by_paragraphs(text)\n",
        "    summaries = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        inputs = tokenizer(f\"Below is a political text. Summarize it in a few sentences and return only the summary.\\n\\nText:\\n\\n{chunk}\\n\\n. The summary of this text:\", return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=150,\n",
        "            num_beams=1,\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=4,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        # Strip the summary so only the part after the \"Text:\" is kept\n",
        "        if \"Text:\" in decoded:\n",
        "          summary = decoded.split(\"The summary of this text:\")[-1].strip()\n",
        "        else:\n",
        "          summary = decoded.strip()\n",
        "        summaries.append(summary)\n",
        "    return summaries\n",
        "\n",
        "# Final summarization\n",
        "def summarize_overall(model, tokenizer, summaries, device='cuda'):\n",
        "    combined = ' '.join(summaries)\n",
        "    inputs = tokenizer(f\"Below is a political text. Summarize it in a few sentences and return only the summary.\\n\\nText:\\n\\n{combined}\\n\\n. The summary of this text:\", return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=150,\n",
        "        num_beams=1,\n",
        "        repetition_penalty=1.5,\n",
        "        no_repeat_ngram_size=4,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    decoded = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    # Strip the summary so only the part after the \"Text:\" is kept\n",
        "    if \"Text:\" in decoded:\n",
        "      summary = decoded.split(\"The summary of this text:\")[-1].strip()\n",
        "    else:\n",
        "      summary = decoded.strip()\n",
        "    return summary\n",
        "\n",
        "def generate_summary(text):\n",
        "  summaries = summarize_paragraphs(model, tokenizer, text)\n",
        "  final_summary = summarize_overall(model, tokenizer, summaries)\n",
        "  return final_summary\n"
      ],
      "metadata": {
        "id": "Rj_ZkRQfG8S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the necessary metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "meteor = evaluate.load(\"meteor\")"
      ],
      "metadata": {
        "id": "x_5yNeLjTjn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the benchmark pre fine-tuned model\n",
        "model_name = \"google/gemma-3-1b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = Gemma3ForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16).to(device)"
      ],
      "metadata": {
        "id": "7hefwXUyaL1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_dataset['text'][1]"
      ],
      "metadata": {
        "id": "ZQXdC1sHOzxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the summarization function on the holdout set\n",
        "holdout_summaries = [generate_summary(text) for text in holdout_dataset['text'][1]]"
      ],
      "metadata": {
        "id": "CA4wWv8XO9aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_summaries[1]"
      ],
      "metadata": {
        "id": "daajbzORyk0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_dataset['summary'][1]"
      ],
      "metadata": {
        "id": "LhfPZ9-gcsC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROUGE\n",
        "rouge_score = rouge.compute(predictions=holdout_summaries, references=holdout_dataset['summary'])\n",
        "print(\"ROUGE Score:\", rouge_score)"
      ],
      "metadata": {
        "id": "GxlNtN1UdqeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute BLEU\n",
        "bleu_predictions = [summary for summary in holdout_summaries]\n",
        "bleu_references = [[ref] for ref in holdout_dataset['summary']]\n",
        "\n",
        "bleu_score = bleu.compute(predictions=bleu_predictions, references=bleu_references)\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "metadata": {
        "id": "saUJE5audqgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute METEOR\n",
        "meteor_score = meteor.compute(predictions=holdout_summaries, references=holdout_dataset['summary'])\n",
        "print(\"METEOR Score:\", meteor_score)"
      ],
      "metadata": {
        "id": "i7Adwgycdqik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize dataset\n",
        "model_name = \"google/gemma-3-1b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map='auto')\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Perform the 80-20 train-test split\n",
        "train_test_split = tokenized_train_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "validation_dataset = train_test_split['test']"
      ],
      "metadata": {
        "id": "YAu90v6uVPxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-05, 2e-05, 3e-05, 5e-05]\n",
        "batch_sizes = [4, 8]\n",
        "combinations = [(lr, bs) for lr in learning_rates for bs in batch_sizes]\n",
        "\n",
        "model_name = \"google/gemma-3-1b-it\"\n",
        "\n",
        "for lr, bs in combinations:\n",
        "  print(f\"Learning Rate: {lr} and Batch Size: {bs} is running\")\n",
        "  try:\n",
        "    del model\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del trainer\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del training_args\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del tokenizer\n",
        "  except:\n",
        "    pass\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "  save_path = f\"/content/drive/My Drive/my_summarizer_model/lora_finetuned_model_{lr}_{bs}\"\n",
        "\n",
        "  # Load Tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.save_pretrained(save_path)\n",
        "\n",
        "  # Configure LoRA\n",
        "  lora_config = LoraConfig(\n",
        "      r=8,\n",
        "      lora_alpha=32,\n",
        "      lora_dropout=0.1,\n",
        "      # target_modules=\"all-linear\",\n",
        "      target_modules=[\"q_proj\", \"v_proj\"],\n",
        "      task_type=\"CAUSAL_LM\"\n",
        "      )\n",
        "\n",
        "  model = Gemma3ForCausalLM.from_pretrained(model_name,\n",
        "                                            attn_implementation=\"eager\",\n",
        "                                            device_map=\"auto\",\n",
        "                                            torch_dtype=torch.float16)\n",
        "\n",
        "  # Reduce memory usage\n",
        "  model.config.use_cache = False\n",
        "  model.gradient_checkpointing_enable()\n",
        "\n",
        "  model = get_peft_model(model, lora_config)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=save_path,\n",
        "      learning_rate=lr,\n",
        "      per_device_train_batch_size=bs,\n",
        "      per_device_eval_batch_size=4,\n",
        "      gradient_accumulation_steps=1,\n",
        "      num_train_epochs=4,\n",
        "      weight_decay=0.01,\n",
        "      save_strategy=\"best\",\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"eval_meteor\",\n",
        "      greater_is_better=True,\n",
        "      save_total_limit=1,\n",
        "      fp16=True,\n",
        "      label_names=[\"labels\"],\n",
        "      report_to=\"none\",\n",
        "      optim=\"adamw_torch_fused\"\n",
        "  )\n",
        "\n",
        "  trainer = SFTTrainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=validation_dataset,\n",
        "      peft_config=lora_config\n",
        "  )\n",
        "\n",
        "  torch.cuda.reset_peak_memory_stats()\n",
        "  log_resources(\"Before training\")\n",
        "  start_time = time.time()\n",
        "  # Start training\n",
        "  trainer.train()\n",
        "\n",
        "  end_time = time.time()\n",
        "  log_resources(\"After training\")\n",
        "  peak_alloc = torch.cuda.max_memory_allocated() / 1e6\n",
        "  peak_reserved = torch.cuda.max_memory_reserved() / 1e6\n",
        "\n",
        "  print(f\"Peak GPU Allocated: {peak_alloc:.2f} MB\")\n",
        "  print(f\"Peak GPU Reserved:  {peak_reserved:.2f} MB\")\n",
        "  print(f\"Elapsed Time: {round(end_time - start_time, 2)} seconds\")\n",
        "\n",
        "  trainer.save_model()\n",
        "\n",
        "  # Final cleanup\n",
        "  try:\n",
        "      del model, trainer, training_args, tokenizer\n",
        "  except:\n",
        "      pass\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "aNCYTMJbd5Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-05, 2e-05, 3e-05, 5e-05]\n",
        "batch_sizes = [4, 8]\n",
        "combinations = [(lr, bs) for lr in learning_rates for bs in batch_sizes]\n",
        "\n",
        "try:\n",
        "  del model\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  del trainer\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  del training_args\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  del tokenizer\n",
        "except:\n",
        "  pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "evaluation = pd.DataFrame(columns=['model', 'ROUGE Score', 'BLEU Score', 'METEOR Score'])\n",
        "for lr, bs in combinations:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/lora_finetuned_model_{lr}_{bs}\")\n",
        "  model = Gemma3ForCausalLM.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/lora_finetuned_model_{lr}_{bs}\", device_map=\"auto\", torch_dtype=torch.float16).to(device)\n",
        "  test_summaries = [generate_summary(text) for text in holdout_dataset['text']]\n",
        "  rouge_score = rouge.compute(predictions=test_summaries, references=holdout_dataset['summary'])\n",
        "  bleu_predictions = [summary for summary in test_summaries]\n",
        "  bleu_references = [[ref] for ref in holdout_dataset['summary']]\n",
        "  bleu_score = bleu.compute(predictions=bleu_predictions, references=bleu_references)\n",
        "  meteor_score = meteor.compute(predictions=test_summaries, references=holdout_dataset['summary'])\n",
        "  evaluation.loc[len(evaluation)] = [f'lora_finetuned_model_{lr}_{bs}', rouge_score, bleu_score, meteor_score]"
      ],
      "metadata": {
        "id": "0W-Osd-aWM6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.head(12)"
      ],
      "metadata": {
        "id": "7faYTgSV-kph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-05, 2e-05, 3e-05, 5e-05]\n",
        "batch_sizes = [4, 8]\n",
        "combinations = [(lr, bs) for lr in learning_rates for bs in batch_sizes]\n",
        "\n",
        "\n",
        "model_name = \"google/gemma-3-1b-it\"\n",
        "\n",
        "for lr, bs in combinations:\n",
        "  print(f\"Learning Rate: {lr} and Batch Size: {bs} is running\")\n",
        "\n",
        "  try:\n",
        "    del model\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del trainer\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del training_args\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del tokenizer\n",
        "  except:\n",
        "    pass\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  save_path = f\"/content/drive/My Drive/my_summarizer_model/qlora_finetuned_model_{lr}_{bs}\"\n",
        "\n",
        "  if bs == 8:\n",
        "    bs = 4\n",
        "    gas = 2\n",
        "  else: gas = 1\n",
        "\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_storage=torch.float16\n",
        "    )\n",
        "\n",
        "  # Load Tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.save_pretrained(save_path)\n",
        "\n",
        "  # Configure QLoRA\n",
        "  lora_config = LoraConfig(\n",
        "      r=8,\n",
        "      lora_alpha=32,\n",
        "      lora_dropout=0.1,\n",
        "      # target_modules=\"all-linear\",\n",
        "      target_modules=[\"q_proj\", \"v_proj\"],\n",
        "      task_type=\"CAUSAL_LM\"\n",
        "      )\n",
        "\n",
        "  model = Gemma3ForCausalLM.from_pretrained(model_name,\n",
        "                                            attn_implementation=\"eager\",\n",
        "                                            device_map=\"auto\",\n",
        "                                            quantization_config=bnb_config,\n",
        "                                            torch_dtype=torch.float16)\n",
        "\n",
        "    # Reduce memory usage\n",
        "  model.config.use_cache = False\n",
        "  model.gradient_checkpointing_enable()\n",
        "\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  model = get_peft_model(model, lora_config)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=save_path,\n",
        "      learning_rate=lr,\n",
        "      per_device_train_batch_size=bs,\n",
        "      per_device_eval_batch_size=4,\n",
        "      gradient_accumulation_steps=gas,\n",
        "      num_train_epochs=4,\n",
        "      weight_decay=0.01,\n",
        "      save_strategy=\"best\",\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"eval_meteor\",\n",
        "      greater_is_better=True,\n",
        "      save_total_limit=1,\n",
        "      fp16=True,\n",
        "      label_names=[\"labels\"],\n",
        "      report_to=\"none\",\n",
        "      optim=\"adamw_torch_fused\"\n",
        "  )\n",
        "\n",
        "  trainer = SFTTrainer(\n",
        "      model=model.to(device),\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=validation_dataset,\n",
        "      peft_config=lora_config\n",
        "  )\n",
        "\n",
        "  torch.cuda.reset_peak_memory_stats()\n",
        "  log_resources(\"Before training\")\n",
        "  start_time = time.time()\n",
        "  # Start training\n",
        "  trainer.train()\n",
        "\n",
        "  end_time = time.time()\n",
        "  log_resources(\"After training\")\n",
        "  peak_alloc = torch.cuda.max_memory_allocated() / 1e6\n",
        "  peak_reserved = torch.cuda.max_memory_reserved() / 1e6\n",
        "\n",
        "  print(f\"Peak GPU Allocated: {peak_alloc:.2f} MB\")\n",
        "  print(f\"Peak GPU Reserved:  {peak_reserved:.2f} MB\")\n",
        "  print(f\"Elapsed Time: {round(end_time - start_time, 2)} seconds\")\n",
        "\n",
        "  trainer.save_model()\n",
        "\n",
        "  # Final cleanup\n",
        "  try:\n",
        "      del model, trainer, training_args, tokenizer\n",
        "  except:\n",
        "      pass\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "j9-zHdcsDVqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-05, 2e-05, 3e-05, 5e-05]\n",
        "batch_sizes = [4, 8]\n",
        "combinations = [(lr, bs) for lr in learning_rates for bs in batch_sizes]\n",
        "\n",
        "try:\n",
        "  del model\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  del trainer\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  del training_args\n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  del tokenizer\n",
        "except:\n",
        "  pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "evaluation = pd.DataFrame(columns=['model', 'ROUGE Score', 'BLEU Score', 'METEOR Score'])\n",
        "for lr, bs in combinations:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/qlora_finetuned_model_{lr}_{bs}\")\n",
        "  model = Gemma3ForCausalLM.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/qlora_finetuned_model_{lr}_{bs}\", device_map=\"auto\", torch_dtype=torch.float16).to(device)\n",
        "  test_summaries = [generate_summary(text) for text in holdout_dataset['text']]\n",
        "  rouge_score = rouge.compute(predictions=test_summaries, references=holdout_dataset['summary'])\n",
        "  bleu_predictions = [summary for summary in test_summaries]\n",
        "  bleu_references = [[ref] for ref in holdout_dataset['summary']]\n",
        "  bleu_score = bleu.compute(predictions=bleu_predictions, references=bleu_references)\n",
        "  meteor_score = meteor.compute(predictions=test_summaries, references=holdout_dataset['summary'])\n",
        "  evaluation.loc[len(evaluation)] = [f'qlora_finetuned_model_{lr}_{bs}', rouge_score, bleu_score, meteor_score]"
      ],
      "metadata": {
        "id": "n6Oz4QPOFvmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.head(12)"
      ],
      "metadata": {
        "id": "lWpHa-_WdXCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data for LoRA\n",
        "lora_data = [\n",
        "    {\"Learning Rate\": \"1e-05\", \"Batch Size\": 4, \"Peak GPU Allocated\": 8603.63, \"Before GPU Reserved\": 2657.1, \"After GPU Reserved\": 10812.9, \"Training Time\": 278.79},\n",
        "    {\"Learning Rate\": \"1e-05\", \"Batch Size\": 8, \"Peak GPU Allocated\": 15180.35, \"Before GPU Reserved\": 2699.0, \"After GPU Reserved\": 15321.8, \"Training Time\": 285.57},\n",
        "    {\"Learning Rate\": \"2e-05\", \"Batch Size\": 4, \"Peak GPU Allocated\": 10746.97, \"Before GPU Reserved\": 2699.0, \"After GPU Reserved\": 11882.5, \"Training Time\": 276.52},\n",
        "    {\"Learning Rate\": \"2e-05\", \"Batch Size\": 8, \"Peak GPU Allocated\": 15180.35, \"Before GPU Reserved\": 2699.0, \"After GPU Reserved\": 15300.8, \"Training Time\": 283.17},\n",
        "    {\"Learning Rate\": \"3e-05\", \"Batch Size\": 4, \"Peak GPU Allocated\": 8603.63, \"Before GPU Reserved\": 2699.0, \"After GPU Reserved\": 10812.9, \"Training Time\": 275.28},\n",
        "    {\"Learning Rate\": \"3e-05\", \"Batch Size\": 8, \"Peak GPU Allocated\": 15180.35, \"Before GPU Reserved\": 2699.0, \"After GPU Reserved\": 15279.8, \"Training Time\": 293.81},\n",
        "    {\"Learning Rate\": \"5e-05\", \"Batch Size\": 4, \"Peak GPU Allocated\": 8603.63, \"Before GPU Reserved\": 2699.0, \"After GPU Reserved\": 10812.9, \"Training Time\": 276.42},\n",
        "    {\"Learning Rate\": \"5e-05\", \"Batch Size\": 8, \"Peak GPU Allocated\": 15180.35, \"Before GPU Reserved\": 2699.0, \"After GPU Reserved\": 15300.8, \"Training Time\": 290.73},\n",
        "]\n",
        "\n",
        "# Data for QLoRA\n",
        "qlora_data = [\n",
        "    {\"Learning Rate\": \"1e-05\", \"Batch Size\": 4, \"Peak GPU Allocated\": 8902.32, \"Before GPU Reserved\": 2487.2, \"After GPU Reserved\": 10244.6, \"Training Time\": 348.57},\n",
        "    {\"Learning Rate\": \"1e-05\", \"Batch Size\": 8, \"Peak GPU Allocated\": 8905.35, \"Before GPU Reserved\": 2508.2, \"After GPU Reserved\": 10139.7, \"Training Time\": 344.11},\n",
        "    {\"Learning Rate\": \"2e-05\", \"Batch Size\": 4, \"Peak GPU Allocated\": 8902.32, \"Before GPU Reserved\": 2508.2, \"After GPU Reserved\": 10139.7, \"Training Time\": 354.15},\n",
        "    {\"Learning Rate\": \"2e-05\", \"Batch Size\": 8, \"Peak GPU Allocated\": 8905.35, \"Before GPU Reserved\": 2508.2, \"After GPU Reserved\": 10139.7, \"Training Time\": 344.15},\n",
        "    {\"Learning Rate\": \"3e-05\", \"Batch Size\": 4, \"Peak GPU Allocated\": 8902.32, \"Before GPU Reserved\": 2508.2, \"After GPU Reserved\": 10139.7, \"Training Time\": 354.46},\n",
        "    {\"Learning Rate\": \"3e-05\", \"Batch Size\": 8, \"Peak GPU Allocated\": 9988.56, \"Before GPU Reserved\": 2508.2, \"After GPU Reserved\": 10139.7, \"Training Time\": 344.06},\n",
        "    {\"Learning Rate\": \"5e-05\", \"Batch Size\": 4, \"Peak GPU Allocated\": 8902.32, \"Before GPU Reserved\": 2508.2, \"After GPU Reserved\": 10139.7, \"Training Time\": 354.19},\n",
        "    {\"Learning Rate\": \"5e-05\", \"Batch Size\": 8, \"Peak GPU Allocated\": 9985.58, \"Before GPU Reserved\": 2508.2, \"After GPU Reserved\": 10139.7, \"Training Time\": 344.2},\n",
        "]"
      ],
      "metadata": {
        "id": "h4A2_b8QarAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrames for LoRA and QLoRA\n",
        "df_lora = pd.DataFrame(lora_data)\n",
        "df_qlora = pd.DataFrame(qlora_data)"
      ],
      "metadata": {
        "id": "Afyf4yAqavJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate GPU Reserved Difference\n",
        "df_lora[\"GPU Reserved Difference\"] = df_lora[\"After GPU Reserved\"] - df_lora[\"Before GPU Reserved\"]\n",
        "df_qlora[\"GPU Reserved Difference\"] = df_qlora[\"After GPU Reserved\"] - df_qlora[\"Before GPU Reserved\"]"
      ],
      "metadata": {
        "id": "PSceiPoZawHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lora[\"Hyperparameter combination\"] = pd.concat([df_lora[\"Learning Rate\"], df_lora[\"Batch Size\"]], axis=1).apply(lambda x: f\"{x[0]}, {x[1]}\", axis=1)\n",
        "df_qlora[\"Hyperparameter combination\"] = pd.concat([df_qlora[\"Learning Rate\"], df_qlora[\"Batch Size\"]], axis=1).apply(lambda x: f\"{x[0]}, {x[1]}\", axis=1)"
      ],
      "metadata": {
        "id": "neylNah1dhnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant columns\n",
        "df_lora = df_lora[[\"Hyperparameter combination\", \"Peak GPU Allocated\", \"GPU Reserved Difference\", \"Training Time\"]]\n",
        "df_qlora = df_qlora[[\"Hyperparameter combination\", \"Peak GPU Allocated\", \"GPU Reserved Difference\", \"Training Time\"]]"
      ],
      "metadata": {
        "id": "dyQmc4rXaxFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(14, 12))\n",
        "\n",
        "# Peak GPU Usage\n",
        "sns.lineplot(x=\"Hyperparameter combination\", y=\"Peak GPU Allocated\", data=df_lora, label=\"LoRA\", marker='o', ax=ax[0])\n",
        "sns.lineplot(x=\"Hyperparameter combination\", y=\"Peak GPU Allocated\", data=df_qlora, label=\"QLoRA\", marker='x', ax=ax[0])\n",
        "ax[0].set_title(\"Peak GPU Usage Comparison\")\n",
        "ax[0].set_xlabel(\"Hyperparameter Combination (Learning Rate, Batch Size)\")\n",
        "ax[0].set_ylabel(\"Peak GPU Allocated (MB)\")\n",
        "ax[0].tick_params(axis='x', rotation=45)\n",
        "ax[0].grid(alpha=0.3)\n",
        "\n",
        "# GPU Reserved Difference\n",
        "sns.lineplot(x=\"Hyperparameter combination\", y=\"GPU Reserved Difference\", data=df_lora, label=\"LoRA\", marker='o', ax=ax[1])\n",
        "sns.lineplot(x=\"Hyperparameter combination\", y=\"GPU Reserved Difference\", data=df_qlora, label=\"QLoRA\", marker='x', ax=ax[1])\n",
        "ax[1].set_title(\"GPU Reserved Difference Comparison\")\n",
        "ax[1].set_xlabel(\"Hyperparameter Combination (Learning Rate, Batch Size)\")\n",
        "ax[1].set_ylabel(\"GPU Reserved Difference (MB)\")\n",
        "ax[1].tick_params(axis='x', rotation=45)\n",
        "ax[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U3V1rjgbazxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Training Time Comparison\n",
        "sns.lineplot(x=\"Hyperparameter combination\", y=\"Training Time\", data=df_lora, label=\"LoRA\", marker='o')\n",
        "sns.lineplot(x=\"Hyperparameter combination\", y=\"Training Time\", data=df_qlora, label=\"QLoRA\", marker='x')\n",
        "ax.set_title(\"Training Time Comparison\")\n",
        "ax.set_xlabel(\"Hyperparameter Combination (Learning Rate, Batch Size)\")\n",
        "ax.set_ylabel(\"Training Time (seconds)\")\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zkJy5OHGfBpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/lora_finetuned_model_3e-05_4\")\n",
        "model = Gemma3ForCausalLM.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/lora_finetuned_model_3e-05_4\", device_map=\"auto\", torch_dtype=torch.float16).to(device)\n",
        "lora_summaries = [generate_summary(text) for text in holdout_dataset['text']]"
      ],
      "metadata": {
        "id": "HoFHS09HHvyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_summaries[1]"
      ],
      "metadata": {
        "id": "z0wEf3_KVd4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/qlora_finetuned_model_5e-05_4\")\n",
        "model = Gemma3ForCausalLM.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/qlora_finetuned_model_5e-05_4\", device_map=\"auto\", torch_dtype=torch.float16).to(device)\n",
        "qlora_summaries = [generate_summary(text) for text in holdout_dataset['text']]"
      ],
      "metadata": {
        "id": "0Q1oXHxzJAaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qlora_summaries[1]"
      ],
      "metadata": {
        "id": "UBP-cirdVdUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/lora_finetuned_model_3e-05_4\")\n",
        "model = Gemma3ForCausalLM.from_pretrained(f\"/content/drive/My Drive/my_summarizer_model/lora_finetuned_model_3e-05_4\", device_map=\"auto\", torch_dtype=torch.float16).to(device)"
      ],
      "metadata": {
        "id": "QW2HOQPovBju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=generate_summary,\n",
        "    inputs=gr.Textbox(lines=10, label=\"Enter text to summarize\"),\n",
        "    outputs=gr.Textbox(label=\"Summary\"),\n",
        "    title=\"Text Summarizer\",\n",
        "    description=\"Enter a paragraph and the model will generate a summary.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "fo5dlMFeaeQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7d3BQ_liaetO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}