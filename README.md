# nlp_thesis
The initial task is to create and fine-tune NLP models from Hugging Face in order to summarize texts in a certain domain.
Bart-large-cnn model was used for the task with Full and LoRA fine-tuning.
The data used for fine-tuning comes from: https://www.kaggle.com/datasets/pariza/bbc-news-summary.

Code and Resources Used
For the code there are several Python packages used and all the model usage are done in Google Colaboratory. The packages used can be accessed at requirements.txt
